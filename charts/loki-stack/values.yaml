---
# Configure loki-stack chart with correct structure
loki-stack:
  # Configure loki within the stack
  loki:
    # Force deployment on the worker node with our PV
    nodeSelector:
      kubernetes.io/hostname: worker0

    # Add tolerations to ensure it doesn't run on control plane
    tolerations: []

    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        kubernetes.io/ingress.class: nginx
      hosts:
        - host: loki.local
          paths:
            - /
      pathType: Prefix
    # Optimize Loki for faster indexing and use MinIO for storage
    config:
      schema_config:
        configs:
          - from: "2020-10-24"
            store: boltdb-shipper
            object_store: s3
            schema: v11
            index:
              prefix: loki_index_
              period: 24h
      storage_config:
        boltdb_shipper:
          active_index_directory: /data/loki/index
          shared_store: s3
          cache_location: /data/loki/boltdb-shipper-cache
          cache_ttl: 24h # Increase cache TTL to improve query performance
        aws:
          s3: s3://minioadmin:minioadmin@minio-storage.web3.svc.cluster.local:9000/loki-chunks
          s3forcepathstyle: true
          bucketnames: loki-chunks
          endpoint: minio-storage.web3.svc.cluster.local:9000
          access_key_id: minioadmin
          secret_access_key: minioadmin
          insecure: true
      chunk_store_config:
        max_look_back_period: 0s # Disable lookback period to index everything
      table_manager:
        retention_deletes_enabled: false
        retention_period: 0s
      limits_config:
        ingestion_rate_mb: 10  # Increase ingestion rate (default is 4MB)
        ingestion_burst_size_mb: 20  # Increase burst size (default is 6MB)
        max_global_streams_per_user: 10000  # Increase max streams
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        max_entries_limit_per_query: 10000
      ingester:
        chunk_idle_period: 1m  # Reduced from 3m
        chunk_block_size: 262144
        chunk_retain_period: 30s  # Reduced from 1m
        lifecycler:
          ring:
            kvstore:
              store: inmemory
            replication_factor: 1
      frontend_worker:
        parallelism: 2  # Increase query parallelism

    env:
      - name: AWS_ACCESS_KEY_ID
        value: minioadmin
      - name: AWS_SECRET_ACCESS_KEY
        value: minioadmin

    # Add affinity to prefer worker nodes
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - worker0
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: DoesNotExist

    persistence:
      enabled: true
      size: 50Gi
      storageClassName: local-storage

  # Configure promtail within the stack
  promtail:
    enabled: true
    config:
      clients:
        - url: http://{{ .Release.Name }}:3100/loki/api/v1/push

  # Configure Grafana within the stack
  grafana:
    enabled: true
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 300m
        memory: 256Mi

    # Force deployment on the worker node with our PV
    nodeSelector:
      kubernetes.io/hostname: worker0

    # Default admin credentials
    adminUser: admin
    adminPassword: web3-admin-pw

    persistence:
      enabled: true
      size: 50Gi
      storageClassName: local-storage

    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        kubernetes.io/ingress.class: nginx
      hosts:
        - grafana.local
      path: /

    # Configure Loki as a datasource
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            url: http://{{ .Release.Name }}:3100
            access: proxy
            isDefault: true
          - name: Prometheus
            type: prometheus
            url: http://{{ .Release.Name }}-prometheus-server
            access: proxy
            isDefault: false

    # Dashboards to provision
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default

# Configure Prometheus within the stack
  prometheus:
    enabled: true
    server:
      # Force deployment on the worker node with our PV
      nodeSelector:
        kubernetes.io/hostname: worker0

      # Resources for Prometheus server
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 500m
          memory: 1Gi

      # Use persistent storage
      persistentVolume:
        enabled: true
        size: 50Gi
        storageClassName: local-storage

      # Configure Prometheus for better performance
      retention: 7d  # Keep data for 7 days

      # Add affinity to prefer worker nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                      - worker0

      # Fixed ingress format for Prometheus chart
      ingress:
        enabled: true
        ingressClassName: nginx
        annotations:
          kubernetes.io/ingress.class: nginx
        hosts:
          - prometheus.local
        paths:
          - /  # Top-level paths array
        pathType: Prefix

# Configure Prometheus alertmanager
  alertmanager:
    enabled: true

    # Force deployment on the worker node with our PV
    nodeSelector:
      kubernetes.io/hostname: worker0

    # Use persistent storage
    persistentVolume:
      enabled: true
      size: 10Gi
      storageClassName: local-storage

    # Configure ingress for alertmanager
    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        kubernetes.io/ingress.class: nginx
      hosts:
        - host: alertmanager.local
          paths:
            - /
      pathType: Prefix


# Init job configuration for PV setup
initJob:
  enabled: true
  image: "busybox:latest"
  imagePullPolicy: "IfNotPresent"

  # Commands to run for initializing volumes
  args:
    - |
      # Create Loki directories with appropriate permissions
      # Main directories
      mkdir -p /storage/loki/write
      mkdir -p /storage/loki/backend

      # Create subdirectories that Loki expects
      mkdir -p /storage/loki/write/data/loki
      mkdir -p /storage/loki/write/data/loki/boltdb-shipper-active
      mkdir -p /storage/loki/write/data/loki/boltdb-shipper-cache
      mkdir -p /storage/loki/write/data/loki/chunks
      mkdir -p /storage/loki/write/data/loki/index

      # Setting very permissive permissions to avoid issues
      chmod -R 777 /storage/loki

      # Create Grafana directory with appropriate permissions
      mkdir -p /storage/grafana
      # Grafana runs as user 472
      chmod -R 777 /storage/grafana

      # Create Prometheus directories with appropriate permissions
      mkdir -p /storage/prometheus
      # Prometheus runs as nobody (UID 65534)
      chown -R 65534:65534 /storage/prometheus
      chmod -R 755 /storage/prometheus

      # Create AlertManager directories with appropriate permissions
      mkdir -p /storage/alertmanager
      # AlertManager also runs as nobody (UID 65534)
      chown -R 65534:65534 /storage/alertmanager
      chmod -R 755 /storage/alertmanager

      echo "Loki stack volume initialization completed successfully!"

templates:
  pv: true  # Enable PVs for local-storage (required)