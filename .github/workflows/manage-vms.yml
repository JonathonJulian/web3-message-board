name: Manage vSphere VMs
run-name: ${{ github.event.inputs.action }} VM ${{ github.event.inputs.vm_name }}${{ github.event.inputs.rke2_worker == 'true' && ' as RKE2 worker' || '' }}

on:
  workflow_dispatch:
    inputs:
      action:
        description: "Action to perform"
        required: true
        default: "add"
        type: choice
        options:
          - add
          - remove
          - update
      vm_name:
        description: "VM name (required for any operation)"
        required: true
        type: string
      ip_address:
        description: "Static IP address (required)"
        required: false
        type: string
      subnet_mask:
        description: "Subnet mask prefix length (e.g. 24 for /24)"
        required: false
        default: "24"
        type: string
      ssh_public_key:
        description: "SSH public key for VM access"
        required: false
        type: string
      disk_size_gb:
        description: "VM disk size in GB"
        required: false
        default: 20
        type: number
      storage_class:
        description: "Storage class for VM disks"
        required: false
        default: "SSD"
        type: choice
        options:
          - SSD
          - NVME
          - SATA
      cpu:
        description: "Number of CPU cores"
        required: false
        default: 2
        type: number
      memory:
        description: "Memory in GB"
        required: false
        default: 4
        type: number
      rke2_worker:
        description: "Add as RKE2 worker node"
        required: false
        default: false
        type: boolean

env:
  # vSphere credentials from secrets
  VSPHERE_SERVER: ${{ secrets.VSPHERE_SERVER }}
  VSPHERE_USER: ${{ secrets.VSPHERE_USER }}
  VSPHERE_PASSWORD: ${{ secrets.VSPHERE_PASSWORD }}

  # MinIO configuration - from secrets or default values
  MINIO_ENDPOINT: ${{ secrets.MINIO_ENDPOINT || 'http://minio.local' }}
  MINIO_ACCESS_KEY: ${{ secrets.MINIO_ACCESS_KEY || 'minioadmin' }}
  MINIO_SECRET_KEY: ${{ secrets.MINIO_SECRET_KEY || 'minioadmin' }}
  MINIO_BUCKET: ${{ secrets.MINIO_BUCKET || 'terraform-state' }}
  MINIO_OBJECT_PATH: "vm-configs.json"

  # AWS credentials for Terraform S3 backend (MinIO)
  AWS_ACCESS_KEY_ID: ${{ secrets.MINIO_ACCESS_KEY || 'minioadmin' }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.MINIO_SECRET_KEY || 'minioadmin' }}
  AWS_EC2_METADATA_DISABLED: true

jobs:
  manage-vms:
    name: ${{ github.event.inputs.action }} VM ${{ github.event.inputs.vm_name }}${{ github.event.inputs.rke2_worker == 'true' && ' as RKE2 worker' || '' }}
    runs-on: self-hosted

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Verify Terraform is installed
        run: |
          terraform --version
          echo "Using existing Terraform installation on the runner"

      - name: Set parameters
        id: params
        run: |
          echo "ACTION=${{ github.event.inputs.action }}" >> $GITHUB_OUTPUT
          echo "VM_NAME=${{ github.event.inputs.vm_name }}" >> $GITHUB_OUTPUT
          echo "NETWORK_TYPE=static" >> $GITHUB_OUTPUT
          echo "IP_ADDRESS=${{ github.event.inputs.ip_address }}" >> $GITHUB_OUTPUT
          echo "SUBNET_MASK=${{ github.event.inputs.subnet_mask }}" >> $GITHUB_OUTPUT
          echo "SSH_PUBLIC_KEY=${{ github.event.inputs.ssh_public_key }}" >> $GITHUB_OUTPUT
          echo "DISK_SIZE_GB=${{ github.event.inputs.disk_size_gb }}" >> $GITHUB_OUTPUT
          echo "STORAGE_CLASS=${{ github.event.inputs.storage_class }}" >> $GITHUB_OUTPUT
          echo "RKE2_WORKER=${{ github.event.inputs.rke2_worker }}" >> $GITHUB_OUTPUT

          # Ensure CPU is at least 2 cores
          CPU="${{ github.event.inputs.cpu }}"
          if [[ -z "$CPU" || "$CPU" -lt 2 ]]; then
            CPU=2
            echo "Setting CPU to minimum required value: 2"
          fi
          echo "CPU=$CPU" >> $GITHUB_OUTPUT

          # Ensure memory is at least 2GB for Ubuntu 24.04 (Noble)
          MEMORY="${{ github.event.inputs.memory }}"
          if [[ -z "$MEMORY" || "$MEMORY" -lt 2 ]]; then
            MEMORY=2
            echo "Setting memory to minimum required value: 2GB"
          fi

          # For RKE2 worker nodes, ensure at least 4GB memory
          if [[ "${{ github.event.inputs.rke2_worker }}" == "true" && "$MEMORY" -lt 4 ]]; then
            MEMORY=4
            echo "Setting memory to minimum required value for RKE2: 4GB"
          fi

          echo "MEMORY=$MEMORY" >> $GITHUB_OUTPUT

      - name: Pull VM configurations from MinIO
        id: pull_configs
        run: |
          # Create the destination directory
          mkdir -p ./terraform/environments/dev

          # Configure MinIO client with proper alias
          echo "Configuring MinIO client..."
          mc alias set myminio http://minio.local minioadmin minioadmin

          # Verify MinIO connectivity and debug output
          echo "Testing MinIO connectivity..."
          mc ls myminio/ || echo "Failed to list MinIO root"

          echo "Listing terraform-state bucket contents..."
          mc ls myminio/terraform-state/ || echo "Failed to list bucket contents"

          # Pull configurations from MinIO using temporary file first
          echo "Pulling configurations from MinIO..."
          TEMP_FILE="/tmp/terraform.tfvars.json"
          FINAL_PATH="$(pwd)/terraform/environments/dev/terraform.tfvars.json"

          if mc cp myminio/terraform-state/vm-configs.json "$TEMP_FILE"; then
            echo "Successfully downloaded from MinIO"
            cp "$TEMP_FILE" "$FINAL_PATH"
          else
            echo "Failed to copy from MinIO, creating default configuration..."
            echo '{"vm_configs": {}}' > "$FINAL_PATH"
          fi

          # Display the current configurations
          echo "Current VM configurations:"
          cat ./terraform/environments/dev/terraform.tfvars.json | jq .

      - name: Run VM management script
        id: vm_management
        run: |
          # Ensure AWS environment variables are set for Terraform
          export AWS_ACCESS_KEY_ID="${{ env.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ env.AWS_SECRET_ACCESS_KEY }}"
          export AWS_EC2_METADATA_DISABLED=true

          # Set Terraform variables from environment
          export TF_VAR_vsphere_server="${{ env.VSPHERE_SERVER }}"
          export TF_VAR_vsphere_user="${{ env.VSPHERE_USER }}"
          export TF_VAR_vsphere_password="${{ env.VSPHERE_PASSWORD }}"
          export TF_VAR_ssh_public_key="${{ steps.params.outputs.SSH_PUBLIC_KEY }}"

          # Print which variables are being used (mask sensitive values)
          echo "Using vSphere server: $TF_VAR_vsphere_server"
          echo "Using vSphere user: $TF_VAR_vsphere_user"
          echo "Using SSH public key: ${TF_VAR_ssh_public_key:0:20}... (truncated)"

          bash ./scripts/manage_vms.sh \
            --action="${{ steps.params.outputs.ACTION }}" \
            --vm-name="${{ steps.params.outputs.VM_NAME }}" \
            --ip-address="${{ steps.params.outputs.IP_ADDRESS }}" \
            --subnet-mask="${{ steps.params.outputs.SUBNET_MASK }}" \
            --ssh-public-key="${{ steps.params.outputs.SSH_PUBLIC_KEY }}" \
            --disk-size-gb="${{ steps.params.outputs.DISK_SIZE_GB }}" \
            --storage-class="${{ steps.params.outputs.STORAGE_CLASS }}" \
            --cpu="${{ steps.params.outputs.CPU }}" \
            --memory="${{ steps.params.outputs.MEMORY }}" \
            --rke2-worker="${{ steps.params.outputs.RKE2_WORKER }}"

          # If we're adding a VM, make sure we also update all other VMs to have SSH keys for consistency
          if [[ "${{ steps.params.outputs.ACTION }}" == "add" || "${{ steps.params.outputs.ACTION }}" == "apply" ]] && [[ -n "${{ steps.params.outputs.SSH_PUBLIC_KEY }}" ]]; then
            echo "Ensuring all VMs have SSH public keys set..."
            # Extract all VM names
            VM_NAMES=$(jq -r '.vm_configs | keys[]' ./terraform/environments/dev/terraform.tfvars.json)
            for VM in $VM_NAMES; do
              # Check if this VM already has an SSH key
              HAS_KEY=$(jq -r ".vm_configs[\"$VM\"].ssh_public_key // \"\"" ./terraform/environments/dev/terraform.tfvars.json)
              if [[ -z "$HAS_KEY" ]]; then
                echo "Adding SSH key to VM '$VM'"
                bash ./scripts/manage_vms.sh \
                  --action="add" \
                  --vm-name="$VM" \
                  --ssh-public-key="${{ steps.params.outputs.SSH_PUBLIC_KEY }}"
              fi
            done
          fi

          # Always run apply after add/remove operations to apply the changes
          if [[ "${{ steps.params.outputs.ACTION }}" == "add" || "${{ steps.params.outputs.ACTION }}" == "remove" ]]; then
            echo "Running apply operation to apply configuration changes..."
            bash ./scripts/manage_vms.sh --action=apply
          fi

          # Save VM configurations for next steps
          if [[ -f "./terraform/environments/dev/terraform.tfvars.json" ]]; then
            echo "Copying configuration for later steps..."
            cp ./terraform/environments/dev/terraform.tfvars.json /tmp/vm_configs.json

            # Push updated config back to MinIO
            echo "Pushing updated configuration back to MinIO..."
            TEMP_FILE="/tmp/terraform-updated.tfvars.json"
            cp ./terraform/environments/dev/terraform.tfvars.json "$TEMP_FILE"

            # Configure MinIO client again in case this is a separate shell session
            echo "Configuring MinIO client for push operation..."
            mc alias set myminio "${{ secrets.MINIO_ENDPOINT || env.MINIO_ENDPOINT }}" "${{ secrets.MINIO_ACCESS_KEY || env.MINIO_ACCESS_KEY }}" "${{ secrets.MINIO_SECRET_KEY || env.MINIO_SECRET_KEY }}"

            # Verify MinIO connectivity before push
            echo "Verifying MinIO connectivity..."
            if ! mc ls myminio/ &>/dev/null; then
              echo "ERROR: Unable to connect to MinIO server at ${{ secrets.MINIO_ENDPOINT || env.MINIO_ENDPOINT }}"
              echo "MinIO connection test failed. Check hostname, credentials, and network connectivity."
              echo "Will save configuration locally but not push to MinIO."
              echo "MINIO_PUSH_STATUS=failed" >> $GITHUB_ENV
            else
              echo "MinIO connection successful. Proceeding with push operation."

              # Verify bucket exists
              echo "Verifying bucket exists: ${{ env.MINIO_BUCKET }}"
              if ! mc ls myminio/"${{ env.MINIO_BUCKET }}/" &>/dev/null; then
                echo "Bucket doesn't exist, attempting to create it..."
                mc mb myminio/"${{ env.MINIO_BUCKET }}/" || {
                  echo "ERROR: Failed to create bucket ${{ env.MINIO_BUCKET }}"
                  echo "MINIO_PUSH_STATUS=failed" >> $GITHUB_ENV
                }
              fi

              # Push the configuration
              echo "Pushing configuration to myminio/${{ env.MINIO_BUCKET }}/${{ env.MINIO_OBJECT_PATH }}..."
              if mc cp "$TEMP_FILE" myminio/"${{ env.MINIO_BUCKET }}"/"${{ env.MINIO_OBJECT_PATH }}"; then
                echo "Successfully pushed configuration to MinIO"
                echo "MINIO_PUSH_STATUS=success" >> $GITHUB_ENV
              else
                echo "ERROR: Failed to push configuration to MinIO"
                echo "MINIO_PUSH_STATUS=failed" >> $GITHUB_ENV
              fi
            fi

            echo "CONFIGS_SAVED=true" >> $GITHUB_ENV
          else
            echo "CONFIGS_SAVED=false" >> $GITHUB_ENV
          fi

      - name: Wait for VMs to come online and verify accessibility
        if: |
          env.CONFIGS_SAVED == 'true' &&
          (steps.params.outputs.action == 'add' || steps.params.outputs.action == 'apply')
        id: wait_for_vms
        run: |
          # Create a status file for the summary
          echo '[]' > /tmp/vm_status.json

          # Get VM configurations
          VM_CONFIGS=$(jq -r '.vm_configs' /tmp/vm_configs.json)

          # Initialize status array
          STATUS_ARRAY=()

          # Initial wait to give VMs time to boot
          echo "Waiting 60 seconds for VMs to complete initialization..."
          sleep 60

          # If this is an add action, only check the VM being added
          if [[ "${{ steps.params.outputs.action }}" == "add" ]]; then
            VM_KEY="${{ steps.params.outputs.VM_NAME }}"

            # Verify the VM exists in configs
            if ! echo "$VM_CONFIGS" | jq -e --arg key "$VM_KEY" '.[$key]' > /dev/null; then
              echo "❌ Error: VM $VM_KEY not found in configuration"
              exit 1
            fi

            VM_NAME=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].name")
            IP_ADDRESS=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].ip_address")
            IS_RKE2=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].is_rke2_worker // \"false\"")

            echo "Checking if VM $VM_NAME ($IP_ADDRESS) is online..."

            # Check SSH port (port 22) without requiring ping
            echo "Attempting to connect to SSH port (22)..."
            if timeout 5 bash -c "</dev/tcp/$IP_ADDRESS/22" &>/dev/null; then
              echo "✅ VM $VM_NAME ($IP_ADDRESS) has SSH port open and is responsive"
              STATUS="Online"
              SSH_STATUS="Available"
            else
              echo "Initial SSH check failed, trying again with longer timeout..."

              # Try more attempts with longer timeouts
              MAX_ATTEMPTS=12
              SUCCESS=false

              for ((i=1; i<=MAX_ATTEMPTS; i++)); do
                echo "TCP connection attempt $i/$MAX_ATTEMPTS for $IP_ADDRESS:22..."
                if timeout 10 bash -c "</dev/tcp/$IP_ADDRESS/22" &>/dev/null; then
                  echo "✅ VM $VM_NAME ($IP_ADDRESS) is online after $i attempts"
                  STATUS="Online"
                  SSH_STATUS="Available"
                  SUCCESS=true
                  break
                fi

                echo "Attempt $i/$MAX_ATTEMPTS: VM $VM_NAME ($IP_ADDRESS) not responding yet"
                sleep 15  # 15 seconds between attempts (total 3 minutes max)
              done

              if [[ "$SUCCESS" == "false" ]]; then
                echo "❌ Could not connect to VM $VM_NAME ($IP_ADDRESS)"
                echo "Note: VM may still be booting or port 22 may not be open yet"
                STATUS="Pending (No Connection)"
                SSH_STATUS="Unavailable"
              fi
            fi

            # Generate SSH command
            SSH_COMMAND="ssh ubuntu@$IP_ADDRESS"

            # Add to status array for summary
            STATUS_ARRAY+=("$(jq -n \
              --arg name "$VM_NAME" \
              --arg key "$VM_KEY" \
              --arg ip "$IP_ADDRESS" \
              --arg status "$STATUS" \
              --arg ssh_status "$SSH_STATUS" \
              --arg ssh_command "$SSH_COMMAND" \
              --arg is_rke2 "$IS_RKE2" \
              '{name: $name, key: $key, ip: $ip, status: $status, ssh_status: $ssh_status, ssh_command: $ssh_command, is_rke2_worker: $is_rke2}')")
          else
            # For apply action, loop through each VM (original behavior)
            for VM_KEY in $(echo "$VM_CONFIGS" | jq -r 'keys[]'); do
              VM_NAME=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].name")
              IP_ADDRESS=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].ip_address")
              IS_RKE2=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].is_rke2_worker // \"false\"")

              echo "Checking if VM $VM_NAME ($IP_ADDRESS) is online..."

              # Check SSH port (port 22) without requiring ping
              echo "Attempting to connect to SSH port (22)..."
              if timeout 5 bash -c "</dev/tcp/$IP_ADDRESS/22" &>/dev/null; then
                echo "✅ VM $VM_NAME ($IP_ADDRESS) has SSH port open and is responsive"
                STATUS="Online"
                SSH_STATUS="Available"
              else
                echo "Initial SSH check failed, trying again with longer timeout..."

                # Try more attempts with longer timeouts
                MAX_ATTEMPTS=12
                SUCCESS=false

                for ((i=1; i<=MAX_ATTEMPTS; i++)); do
                  echo "TCP connection attempt $i/$MAX_ATTEMPTS for $IP_ADDRESS:22..."
                  if timeout 10 bash -c "</dev/tcp/$IP_ADDRESS/22" &>/dev/null; then
                    echo "✅ VM $VM_NAME ($IP_ADDRESS) is online after $i attempts"
                    STATUS="Online"
                    SSH_STATUS="Available"
                    SUCCESS=true
                    break
                  fi

                  echo "Attempt $i/$MAX_ATTEMPTS: VM $VM_NAME ($IP_ADDRESS) not responding yet"
                  sleep 15  # 15 seconds between attempts (total 3 minutes max)
                done

                if [[ "$SUCCESS" == "false" ]]; then
                  echo "❌ Could not connect to VM $VM_NAME ($IP_ADDRESS)"
                  echo "Note: VM may still be booting or port 22 may not be open yet"
                  STATUS="Pending (No Connection)"
                  SSH_STATUS="Unavailable"
                fi
              fi

              # Generate SSH command
              SSH_COMMAND="ssh ubuntu@$IP_ADDRESS"

              # Add to status array for summary
              STATUS_ARRAY+=("$(jq -n \
                --arg name "$VM_NAME" \
                --arg key "$VM_KEY" \
                --arg ip "$IP_ADDRESS" \
                --arg status "$STATUS" \
                --arg ssh_status "$SSH_STATUS" \
                --arg ssh_command "$SSH_COMMAND" \
                --arg is_rke2 "$IS_RKE2" \
                '{name: $name, key: $key, ip: $ip, status: $status, ssh_status: $ssh_status, ssh_command: $ssh_command, is_rke2_worker: $is_rke2}')")
            done
          fi

          # Combine status objects into JSON array and save - fixed to properly format JSON
          # Instead of concatenating strings, use jq to properly build the array
          echo '[]' > /tmp/vm_status.json
          for item in "${STATUS_ARRAY[@]}"; do
            # Add each item to the array one at a time using jq
            jq --argjson item "$item" '. += [$item]' /tmp/vm_status.json > /tmp/vm_status.json.tmp
            mv /tmp/vm_status.json.tmp /tmp/vm_status.json
          done

      - name: Generate summary
        if: always()
        run: |
          echo "# VM Management Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Action Performed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Get the action and parameters from the params step
          ACTION="${{ steps.params.outputs.action }}"
          VM_NAME="${{ steps.params.outputs.vm_name }}"
          NETWORK_TYPE="${{ steps.params.outputs.network_type }}"
          IP_ADDRESS="${{ steps.params.outputs.ip_address }}"
          SUBNET_MASK="${{ steps.params.outputs.subnet_mask }}"
          SSH_PUBLIC_KEY="${{ steps.params.outputs.ssh_public_key }}"
          DISK_SIZE_GB="${{ steps.params.outputs.disk_size_gb }}"
          STORAGE_CLASS="${{ steps.params.outputs.storage_class }}"
          CPU="${{ steps.params.outputs.cpu }}"
          MEMORY="${{ steps.params.outputs.memory }}"
          RKE2_WORKER="${{ steps.params.outputs.RKE2_WORKER }}"

          echo "- **Action:** \`$ACTION\`" >> $GITHUB_STEP_SUMMARY

          if [[ "$ACTION" == "add" ]]; then
            echo "- **VM Added:** \`$VM_NAME\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Network Type:** \`static\`" >> $GITHUB_STEP_SUMMARY
            echo "- **IP Address:** \`$IP_ADDRESS/$SUBNET_MASK\`" >> $GITHUB_STEP_SUMMARY
            echo "- **CPU Cores:** \`$CPU\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Memory (GB):** \`$MEMORY\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Disk Size (GB):** \`$DISK_SIZE_GB\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Storage Class:** \`$STORAGE_CLASS\`" >> $GITHUB_STEP_SUMMARY

            if [[ "$RKE2_WORKER" == "true" ]]; then
              echo "- **RKE2 Worker:** \`Yes\`" >> $GITHUB_STEP_SUMMARY
            fi
          elif [[ "$ACTION" == "remove" ]]; then
            echo "- **VM Removed:** \`$VM_NAME\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Add current VM status if available
          if [[ -f "/tmp/vm_status.json" ]]; then
            echo "## Current VMs" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| VM Key | Name | IP Address | Status | SSH | Access Command | RKE2 Worker |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|------|------------|--------|-----|---------------|-------------|" >> $GITHUB_STEP_SUMMARY

            # Process the status file with better error handling
            if jq -e . /tmp/vm_status.json > /dev/null 2>&1; then
              # Process the status file and add each VM to the table
              jq -r '.[] | "| \(.key) | \(.name) | \(.ip) | \(.status) | \(.ssh_status) | `\(.ssh_command)` | \(if .is_rke2_worker == "true" then "✅" else "❌" end) |"' /tmp/vm_status.json >> $GITHUB_STEP_SUMMARY || {
                echo "| ⚠️ Error | Error processing status file | - | - | - | - | - |" >> $GITHUB_STEP_SUMMARY
                echo "Error details: Invalid JSON format in status file" >> $GITHUB_STEP_SUMMARY
              }
            else
              echo "| ⚠️ Error | Error processing status file | - | - | - | - | - |" >> $GITHUB_STEP_SUMMARY
              echo "Error details: Invalid JSON format in status file" >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Access Instructions" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "For VMs with SSH available:" >> $GITHUB_STEP_SUMMARY
            echo "1. Make sure you have the appropriate SSH key configured" >> $GITHUB_STEP_SUMMARY
            echo "2. Use the access command shown in the table above" >> $GITHUB_STEP_SUMMARY

          elif [[ "${{ env.CONFIGS_SAVED }}" == "true" ]]; then
            echo "## Current VM Configurations" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| VM Key | Name | Network Type | IP Address | CPU | Memory (GB) | RKE2 Worker |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|------|-------------|------------|-----|-------------|-------------|" >> $GITHUB_STEP_SUMMARY

            # Process the configurations with better error handling
            if [[ -f "/tmp/vm_configs.json" ]] && jq -e . /tmp/vm_configs.json > /dev/null 2>&1; then
              # Process the configurations and add each VM to the table
              VM_CONFIGS=$(jq -r '.vm_configs' /tmp/vm_configs.json)
              for VM_KEY in $(echo "$VM_CONFIGS" | jq -r 'keys[]' 2>/dev/null || echo ""); do
                if [[ -n "$VM_KEY" ]]; then
                  VM_NAME=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].name" 2>/dev/null || echo "Unknown")
                  CPU=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].cpu // \"1\"" 2>/dev/null || echo "1")
                  MEMORY=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].memory // \"1\"" 2>/dev/null || echo "1")
                  IS_RKE2=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].is_rke2_worker // \"false\"" 2>/dev/null || echo "false")

                  RKE2_STATUS="❌"
                  if [[ "$IS_RKE2" == "true" ]]; then
                    RKE2_STATUS="✅"
                  fi

                  IP_ADDRESS=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].ip_address" 2>/dev/null || echo "Unknown")
                  SUBNET_MASK=$(echo "$VM_CONFIGS" | jq -r ".[\"$VM_KEY\"].subnet_mask // \"24\"" 2>/dev/null || echo "24")
                  echo "| $VM_KEY | $VM_NAME | static | $IP_ADDRESS/$SUBNET_MASK | $CPU | $MEMORY | $RKE2_STATUS |" >> $GITHUB_STEP_SUMMARY
                fi
              done
            else
              echo "| ⚠️ Error | Error processing configuration file | - | - | - | - | - |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "No VM configurations found." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## MinIO Configuration Storage" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "VM configurations are stored in MinIO at:" >> $GITHUB_STEP_SUMMARY
          echo "- **URL:** \`${{ env.MINIO_ENDPOINT }}/${{ env.MINIO_BUCKET }}/${{ env.MINIO_OBJECT_PATH }}\`" >> $GITHUB_STEP_SUMMARY

          # Add push status if available
          if [[ "${{ env.MINIO_PUSH_STATUS }}" == "success" ]]; then
            echo "- **Status:** ✅ Successfully pushed to MinIO" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ env.MINIO_PUSH_STATUS }}" == "failed" ]]; then
            echo "- **Status:** ❌ Failed to push to MinIO" >> $GITHUB_STEP_SUMMARY
          fi

      # --------------------------------
      # Extract host IPs and scan for SSH keys from status file
      # --------------------------------
      - name: Extract host IPs and scan SSH keys
        if: |
          env.CONFIGS_SAVED == 'true' &&
          steps.params.outputs.action == 'add' &&
          steps.params.outputs.RKE2_WORKER == 'true'
        id: scan-hosts
        run: |
          echo "🔍 Extracting host IP from status file..."

          # Debug: print the content of the status file
          echo "Status file content:"
          cat /tmp/vm_status.json

          # The status file has an array of VM objects with 'ip' field
          VM_IP=$(jq -r '.[0].ip' /tmp/vm_status.json)
          if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
            # Try alternative format - sometimes it's not an array
            VM_IP=$(jq -r '.ip' /tmp/vm_status.json)
            if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
              # Try getting IP from the VM configs file instead
              VM_NAME="${{ steps.params.outputs.VM_NAME }}"
              echo "Looking for IP address of $VM_NAME in configs file..."
              VM_IP=$(jq -r --arg vm "$VM_NAME" '.vm_configs[$vm].ip_address' /tmp/vm_configs.json)
              if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
                echo "Error: Could not retrieve VM IP address from status or configs file"
                echo "Status file content:"
                cat /tmp/vm_status.json
                echo "Configs file content (partial):"
                jq '.vm_configs' /tmp/vm_configs.json
                exit 1
              fi
            fi
          fi

          echo "Found IP: $VM_IP"

          # Create a temporary file to store known_hosts entries
          KNOWN_HOSTS_FILE="known_hosts_entries.txt"
          > $KNOWN_HOSTS_FILE

          # Scan SSH key for the IP
          echo "🔑 Scanning SSH key for host..."
          ssh-keyscan -H "$VM_IP" >> $KNOWN_HOSTS_FILE

          KNOWN_HOSTS=$(cat $KNOWN_HOSTS_FILE)
          echo "known_hosts<<EOF" >> $GITHUB_OUTPUT
          echo "$KNOWN_HOSTS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "✅ Scanned SSH keys for $VM_IP"

      # --------------------------------
      # Setup SSH authentication using action
      # --------------------------------
      - name: Install SSH key
        if: |
          env.CONFIGS_SAVED == 'true' &&
          steps.params.outputs.action == 'add' &&
          steps.params.outputs.RKE2_WORKER == 'true'
        uses: shimataro/ssh-key-action@v2
        with:
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          name: id_ed25519
          known_hosts: ${{ steps.scan-hosts.outputs.known_hosts }}
          if_key_exists: replace

      # --------------------------------
      # Install Ansible and dependencies
      # --------------------------------
      - name: Install Ansible
        if: |
          env.CONFIGS_SAVED == 'true' &&
          steps.params.outputs.action == 'add' &&
          steps.params.outputs.RKE2_WORKER == 'true'
        run: |
          echo "🔧 Installing Ansible and dependencies..."

          # Install Python and Ansible
          if command -v apt-get &>/dev/null; then
            # Ubuntu/Debian
            sudo apt-get update
            sudo apt-get install -y python3 python3-pip
            sudo pip3 install ansible
          elif command -v brew &>/dev/null; then
            # macOS
            brew install ansible
          else
            # Try pip directly as a fallback
            pip3 install --user ansible
            # Add user bin to PATH
            export PATH="$HOME/.local/bin:$PATH"
          fi

          # Verify installation
          ansible --version
          ansible-playbook --version

          # Create necessary directories
          mkdir -p ~/.ansible/roles

      # --------------------------------
      # Run Ansible to configure RKE2 worker node
      # --------------------------------
      - name: Configure RKE2 worker node with Ansible
        if: |
          env.CONFIGS_SAVED == 'true' &&
          steps.params.outputs.action == 'add' &&
          steps.params.outputs.RKE2_WORKER == 'true'
        env:
          ANSIBLE_HOST_KEY_CHECKING: "false"
          RKE2_TOKEN: ${{ secrets.RKE2_TOKEN }}
          RKE2_SERVER_URL: ${{ secrets.RKE2_SERVER_URL }}
        run: |
          echo "🚀 Starting RKE2 worker node configuration..."

          # Debug: print the content of the status file
          echo "Status file content:"
          cat /tmp/vm_status.json

          # Get the VM IP address from the status file
          # The status file has an array of VM objects with 'ip' field
          VM_IP=$(jq -r '.[0].ip' /tmp/vm_status.json)
          if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
            # Try alternative format - sometimes it's not an array
            VM_IP=$(jq -r '.ip' /tmp/vm_status.json)
            if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
              # Try getting IP from the VM configs file instead
              VM_NAME="${{ steps.params.outputs.VM_NAME }}"
              echo "Looking for IP address of $VM_NAME in configs file..."
              VM_IP=$(jq -r --arg vm "$VM_NAME" '.vm_configs[$vm].ip_address' /tmp/vm_configs.json)
              if [ -z "$VM_IP" ] || [ "$VM_IP" == "null" ]; then
                echo "Error: Could not retrieve VM IP address from status or configs file"
                echo "Status file content:"
                cat /tmp/vm_status.json
                echo "Configs file content (partial):"
                jq '.vm_configs' /tmp/vm_configs.json
                exit 1
              fi
            fi
          fi

          echo "VM IP address: $VM_IP"

          # Make sure ansible-playbook is in PATH
          export PATH="$HOME/.local/bin:$PATH"

          # Create Ansible inventory file
          cat > /tmp/ansible_inventory.ini << EOF
          [all]
          worker ansible_host=$VM_IP ansible_user=ubuntu ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          EOF

          # Create a simplified playbook that uses our existing role
          cat > /tmp/add-rke2-worker.yml << EOF
          ---
          # Playbook to add RKE2 worker nodes to an existing cluster
          - name: Add RKE2 worker node
            hosts: all
            become: yes
            gather_facts: yes

            vars:
              rke2_server_url: "{{ lookup('env', 'RKE2_SERVER_URL') }}"
              rke2_token: "{{ lookup('env', 'RKE2_TOKEN') }}"
              rke2_worker_node_labels:
                - "node.kubernetes.io/worker=true"
                - "topology.kubernetes.io/zone=default"

            roles:
              - role: ../../ansible/roles/rke2-worker
          EOF

          # Check if the role exists
          if [ ! -d "ansible/roles/rke2-worker" ]; then
            echo "⚠️ RKE2 worker role not found at ansible/roles/rke2-worker"
            echo "Creating a basic RKE2 worker role directly in the playbook..."

            # Create a playbook with inline tasks instead of using a role
            cat > /tmp/add-rke2-worker.yml << EOF
          ---
          # Playbook to add RKE2 worker nodes to an existing cluster
          - name: Add RKE2 worker node
            hosts: all
            become: yes
            gather_facts: yes

            vars:
              rke2_server_url: "{{ lookup('env', 'RKE2_SERVER_URL') }}"
              rke2_token: "{{ lookup('env', 'RKE2_TOKEN') }}"
              rke2_worker_node_labels:
                - "node.kubernetes.io/worker=true"
                - "topology.kubernetes.io/zone=default"

            tasks:
              - name: Update apt cache
                apt:
                  update_cache: yes
                  cache_valid_time: 3600

              - name: Install required packages
                apt:
                  name:
                    - curl
                    - jq
                  state: present

              - name: Download RKE2 installation script
                get_url:
                  url: https://get.rke2.io
                  dest: /tmp/rke2-install.sh
                  mode: '0755'

              - name: Create RKE2 config directory
                file:
                  path: /etc/rancher/rke2
                  state: directory
                  mode: '0755'

              - name: Configure RKE2 worker
                copy:
                  dest: /etc/rancher/rke2/config.yaml
                  content: |
                    server: {{ rke2_server_url }}
                    token: {{ rke2_token }}
                  mode: '0600'

              - name: Install RKE2 as agent
                shell: INSTALL_RKE2_TYPE=agent /tmp/rke2-install.sh
                args:
                  creates: /usr/local/bin/rke2

              - name: Enable and start RKE2 service
                service:
                  name: rke2-agent
                  enabled: yes
                  state: started

              - name: Wait for RKE2 agent to be ready
                wait_for:
                  path: /var/lib/rancher/rke2/agent/kubelet.kubeconfig
                  state: present
                  timeout: 300
                register: rke2_ready
                ignore_errors: yes

              - name: Check RKE2 agent status
                command: systemctl status rke2-agent
                register: rke2_status
                changed_when: false
                ignore_errors: yes

              - name: Show RKE2 agent status
                debug:
                  var: rke2_status.stdout_lines
          EOF
          fi

          echo "Ansible inventory:"
          cat /tmp/ansible_inventory.ini

          echo "Ansible playbook:"
          cat /tmp/add-rke2-worker.yml

          # Run Ansible playbook with retry logic
          MAX_RETRIES=3
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Running Ansible playbook (attempt $(($RETRY_COUNT + 1))/$MAX_RETRIES)..."

            # Check if ansible-playbook is available
            if ! command -v ansible-playbook &>/dev/null; then
              echo "⚠️ ansible-playbook command not found in PATH"
              echo "Current PATH: $PATH"
              echo "Trying to locate ansible-playbook:"
              find / -name ansible-playbook 2>/dev/null || echo "Not found"

              # Try to install again as a last resort
              pip3 install --user ansible
              export PATH="$HOME/.local/bin:$PATH"
            fi

            # Run with increased verbosity for debugging
            ansible-playbook -i /tmp/ansible_inventory.ini /tmp/add-rke2-worker.yml -vvv

            if [ $? -eq 0 ]; then
              echo "✅ RKE2 worker configuration completed successfully"
              break
            else
              RETRY_COUNT=$(($RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Playbook failed, retrying in 30 seconds..."
                sleep 30
              else
                echo "❌ Failed to configure RKE2 worker after $MAX_RETRIES attempts"
                exit 1
              fi
            fi
          done
